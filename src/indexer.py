"""JERG PDF â†’ ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒãƒ£ãƒ³ã‚¯ â†’ BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""

import json
import re
from pathlib import Path
from pypdf import PdfReader
from fugashi import Tagger

from src.config import WORKING_DIR

DATA_DIR = Path(__file__).parent.parent / "data"
JERG_DIR = DATA_DIR / "jerg"
INDEX_DIR = DATA_DIR / "index"

# ãƒãƒ£ãƒ³ã‚¯è¨­å®š
CHUNK_SIZE = 800
CHUNK_OVERLAP = 100


def extract_text_from_pdf(pdf_path: Path) -> str:
    """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        reader = PdfReader(str(pdf_path))
        pages = []
        for page in reader.pages:
            text = page.extract_text()
            if text:
                pages.append(text)
        return "\n".join(pages)
    except Exception as e:
        print(f"  Warning: {pdf_path.name} ã®èª­ã¿å–ã‚Šã«å¤±æ•—: {e}")
        return ""


def parse_doc_id(filename: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ–‡æ›¸ç•ªå·ã‚’æŠ½å‡º: JAXA-JERG-0-049D.pdf â†’ JERG-0-049"""
    name = filename.replace("JAXA-", "").replace(".pdf", "")
    # æœ«å°¾ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ–‡å­—ã‚’é™¤å» (D, A, B, _N1 ç­‰)
    name = re.sub(r'[A-F]?(_N\d+)?$', '', name)
    return name


def split_into_chunks(text: str, doc_id: str, filename: str) -> list[dict]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸"""
    if not text.strip():
        return []

    # æ®µè½ãƒ»æ–‡ã§åˆ†å‰²ã—ã¦ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã«ã¾ã¨ã‚ã‚‹
    segments = re.split(r'(?<=[ã€‚\n])', text)
    segments = [s for s in segments if s.strip()]

    chunks = []
    current = ""
    chunk_idx = 0

    for seg in segments:
        if len(current) + len(seg) > CHUNK_SIZE and current:
            chunks.append({
                "doc_id": doc_id,
                "filename": filename,
                "chunk_id": f"{doc_id}_{chunk_idx}",
                "text": current.strip(),
            })
            chunk_idx += 1
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: ç¾åœ¨ãƒãƒ£ãƒ³ã‚¯ã®æœ«å°¾ã‚’æ¬¡ã«æŒã¡è¶Šã—
            if len(current) > CHUNK_OVERLAP:
                current = current[-CHUNK_OVERLAP:] + seg
            else:
                current = seg
        else:
            current += seg

    if current.strip():
        chunks.append({
            "doc_id": doc_id,
            "filename": filename,
            "chunk_id": f"{doc_id}_{chunk_idx}",
            "text": current.strip(),
        })

    return chunks


def tokenize_japanese(text: str) -> list[str]:
    """fugashi (MeCab) ã§æ—¥æœ¬èªã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
    tagger = Tagger()
    tokens = []
    for word in tagger(text):
        surface = word.surface
        if len(surface) > 1 or not surface.isascii():
            tokens.append(surface)
    return tokens


def build_index():
    """å…¨JERG PDFã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
    if not JERG_DIR.exists():
        print(f"Error: {JERG_DIR} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    pdf_files = sorted(JERG_DIR.glob("*.pdf"))
    if not pdf_files:
        print(f"Error: {JERG_DIR} ã«PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"ğŸ“š {len(pdf_files)} ä»¶ã®PDFã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­...")

    all_chunks = []
    all_tokenized = []
    tagger = Tagger()

    for i, pdf_path in enumerate(pdf_files, 1):
        doc_id = parse_doc_id(pdf_path.name)
        print(f"  [{i}/{len(pdf_files)}] {pdf_path.name} â†’ {doc_id}")

        text = extract_text_from_pdf(pdf_path)
        if not text:
            continue

        chunks = split_into_chunks(text, doc_id, pdf_path.name)
        for chunk in chunks:
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            tokens = []
            for word in tagger(chunk["text"]):
                surface = word.surface
                if len(surface) > 1 or not surface.isascii():
                    tokens.append(surface)
            all_tokenized.append(tokens)
            all_chunks.append(chunk)

    print(f"\nğŸ“Š åˆè¨ˆ: {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")

    # BM25 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    from rank_bm25 import BM25Okapi
    bm25 = BM25Okapi(all_tokenized)

    # ä¿å­˜
    INDEX_DIR.mkdir(parents=True, exist_ok=True)

    # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    chunks_path = INDEX_DIR / "chunks.json"
    with open(chunks_path, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=1)

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆBM25å†æ§‹ç¯‰ç”¨ï¼‰
    tokens_path = INDEX_DIR / "tokens.json"
    with open(tokens_path, "w", encoding="utf-8") as f:
        json.dump(all_tokenized, f, ensure_ascii=False)

    # æ–‡æ›¸ä¸€è¦§ä¿å­˜
    doc_list = {}
    for chunk in all_chunks:
        did = chunk["doc_id"]
        if did not in doc_list:
            doc_list[did] = {"filename": chunk["filename"], "chunk_count": 0}
        doc_list[did]["chunk_count"] += 1

    doc_list_path = INDEX_DIR / "documents.json"
    with open(doc_list_path, "w", encoding="utf-8") as f:
        json.dump(doc_list, f, ensure_ascii=False, indent=2)

    print(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜å®Œäº†: {INDEX_DIR}")
    print(f"   ãƒãƒ£ãƒ³ã‚¯: {chunks_path}")
    print(f"   æ–‡æ›¸æ•°: {len(doc_list)}")


if __name__ == "__main__":
    build_index()
