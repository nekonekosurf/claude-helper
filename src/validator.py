"""æ–‡æ›¸ãƒ»ãƒŠãƒ¬ãƒƒã‚¸æ•´åˆæ€§æ¤œè¨¼ - å®šæœŸãƒã‚§ãƒƒã‚¯ã¨ç›£æŸ»"""

import json
from pathlib import Path
from src.knowledge import (
    load_routing_rules, list_categories, load_category,
    get_all_knowledge_summary,
)
from src.searcher import get_document_list

INDEX_DIR = Path(__file__).parent.parent / "data" / "index"
JERG_DIR = Path(__file__).parent.parent / "data" / "jerg"


def validate_all() -> dict:
    """å…¨ä½“ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™"""
    results = {
        "pdf_check": check_pdf_files(),
        "index_check": check_index_consistency(),
        "knowledge_check": check_knowledge_consistency(),
        "summary": "",
    }

    # ç·åˆã‚µãƒãƒª
    total_issues = sum(len(r.get("issues", [])) for r in results.values() if isinstance(r, dict))
    total_ok = sum(r.get("ok_count", 0) for r in results.values() if isinstance(r, dict))
    results["summary"] = f"æ¤œè¨¼å®Œäº†: OK={total_ok}, å•é¡Œ={total_issues}"

    return results


def check_pdf_files() -> dict:
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
    issues = []
    ok_count = 0

    if not JERG_DIR.exists():
        return {"ok_count": 0, "issues": ["JERG PDFãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"]}

    pdfs = list(JERG_DIR.glob("*.pdf"))
    ok_count = len(pdfs)

    # ã‚µã‚¤ã‚ºãŒç•°å¸¸ã«å°ã•ã„PDFã‚’æ¤œå‡ºï¼ˆç ´æã®å¯èƒ½æ€§ï¼‰
    for pdf in pdfs:
        size = pdf.stat().st_size
        if size < 1000:
            issues.append(f"ç•°å¸¸ã«å°ã•ã„PDF: {pdf.name} ({size} bytes) - ç ´æã®å¯èƒ½æ€§")

    return {"ok_count": ok_count, "issues": issues, "total_pdfs": len(pdfs)}


def check_index_consistency() -> dict:
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨PDFã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
    issues = []
    ok_count = 0

    chunks_path = INDEX_DIR / "chunks.json"
    doc_list_path = INDEX_DIR / "documents.json"

    if not chunks_path.exists():
        return {"ok_count": 0, "issues": ["ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªæ§‹ç¯‰ã§ã™ã€‚indexer.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"]}

    # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(chunks_path, encoding="utf-8") as f:
        chunks = json.load(f)

    # æ–‡æ›¸ä¸€è¦§èª­ã¿è¾¼ã¿
    doc_list = get_document_list()

    # å„æ–‡æ›¸ã®PDFãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    for doc_id, info in doc_list.items():
        filename = info.get("filename", "")
        pdf_path = JERG_DIR / filename
        if pdf_path.exists():
            ok_count += 1
        else:
            issues.append(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ã‚‹ãŒPDFãŒãªã„: {filename}")

    # ç©ºãƒãƒ£ãƒ³ã‚¯ã®æ¤œå‡º
    empty_chunks = [c for c in chunks if not c.get("text", "").strip()]
    if empty_chunks:
        issues.append(f"ç©ºã®ãƒãƒ£ãƒ³ã‚¯ãŒ {len(empty_chunks)} ä»¶ã‚ã‚Šã¾ã™")

    return {
        "ok_count": ok_count,
        "issues": issues,
        "total_chunks": len(chunks),
        "total_docs": len(doc_list),
    }


def check_knowledge_consistency() -> dict:
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
    issues = []
    ok_count = 0

    rules = load_routing_rules()
    doc_list = get_document_list()
    categories = list_categories()

    # ãƒ«ãƒ¼ãƒ«ã®æ–‡æ›¸å‚ç…§ãƒã‚§ãƒƒã‚¯
    for rule in rules:
        pattern = rule.get("pattern", "")
        cat = rule.get("category", "")

        # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹
        if cat and cat not in categories:
            issues.append(f"ãƒ«ãƒ¼ãƒ« '{pattern}' ã®ã‚«ãƒ†ã‚´ãƒª '{cat}' ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            ok_count += 1

        # å‚ç…§æ–‡æ›¸ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å­˜åœ¨ã™ã‚‹ã‹
        for doc in rule.get("documents", []):
            doc_id = doc.get("id", "")
            # éƒ¨åˆ†ä¸€è‡´ã§ç¢ºèª
            found = any(doc_id in did for did in doc_list.keys())
            if found:
                ok_count += 1
            else:
                issues.append(f"ãƒ«ãƒ¼ãƒ« '{pattern}' ã®å‚ç…§æ–‡æ›¸ '{doc_id}' ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ã‚Šã¾ã›ã‚“")

    return {"ok_count": ok_count, "issues": issues, "total_rules": len(rules)}


def format_report(results: dict) -> str:
    """æ¤œè¨¼çµæœã‚’èª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆã«æ•´å½¢"""
    lines = ["=" * 50, "ğŸ“‹ æ–‡æ›¸ãƒ»ãƒŠãƒ¬ãƒƒã‚¸æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆ", "=" * 50, ""]

    # PDF ãƒã‚§ãƒƒã‚¯
    pdf = results.get("pdf_check", {})
    lines.append(f"## 1. PDFãƒ•ã‚¡ã‚¤ãƒ« ({pdf.get('total_pdfs', 0)} ä»¶)")
    if pdf.get("issues"):
        for issue in pdf["issues"]:
            lines.append(f"  âš ï¸  {issue}")
    else:
        lines.append(f"  âœ… å…¨ {pdf.get('ok_count', 0)} ãƒ•ã‚¡ã‚¤ãƒ«æ­£å¸¸")

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ ãƒã‚§ãƒƒã‚¯
    idx = results.get("index_check", {})
    lines.append(f"\n## 2. æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (æ–‡æ›¸: {idx.get('total_docs', 0)}, ãƒãƒ£ãƒ³ã‚¯: {idx.get('total_chunks', 0)})")
    if idx.get("issues"):
        for issue in idx["issues"]:
            lines.append(f"  âš ï¸  {issue}")
    else:
        lines.append(f"  âœ… å…¨ {idx.get('ok_count', 0)} æ–‡æ›¸ã®æ•´åˆæ€§OK")

    # ãƒŠãƒ¬ãƒƒã‚¸ ãƒã‚§ãƒƒã‚¯
    know = results.get("knowledge_check", {})
    lines.append(f"\n## 3. ãƒŠãƒ¬ãƒƒã‚¸ ({know.get('total_rules', 0)} ãƒ«ãƒ¼ãƒ«)")
    if know.get("issues"):
        for issue in know["issues"]:
            lines.append(f"  âš ï¸  {issue}")
    else:
        lines.append(f"  âœ… å…¨ {know.get('ok_count', 0)} é …ç›®ã®æ•´åˆæ€§OK")

    # ã‚µãƒãƒª
    lines.append(f"\n{'=' * 50}")
    lines.append(results.get("summary", ""))
    lines.append("=" * 50)

    return "\n".join(lines)


def run_validation() -> str:
    """æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã™"""
    results = validate_all()
    return format_report(results)


if __name__ == "__main__":
    print(run_validation())
