"""ãƒãƒ£ãƒ³ã‚¯è¦ç´„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ - LLMã§å„ãƒãƒ£ãƒ³ã‚¯ã‚’è¦ç´„ã—ã€è¦ç´„ã‚‚BM25æ¤œç´¢å¯¾è±¡ã«ã™ã‚‹"""

import json
import time
from pathlib import Path
from rank_bm25 import BM25Okapi
from fugashi import Tagger

from src.llm_client import create_client, chat

INDEX_DIR = Path(__file__).parent.parent / "data" / "index"

SUMMARY_PROMPT = """\
ä»¥ä¸‹ã®æŠ€è¡“æ–‡æ›¸ã®ä¸€éƒ¨ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«50æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
æ—¥å¸¸çš„ãªè¨€è‘‰ã‚’ä½¿ã£ã¦ã€å†…å®¹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
è¦ç´„ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}
"""

_bm25_summary = None
_summaries = None
_chunks = None
_tagger = None


def build_summaries(batch_size: int = 10, max_chunks: int = None):
    """å…¨ãƒãƒ£ãƒ³ã‚¯ã®è¦ç´„ã‚’ç”Ÿæˆã—ã¦ä¿å­˜"""
    chunks_path = INDEX_DIR / "chunks.json"
    if not chunks_path.exists():
        print("Error: chunks.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    with open(chunks_path, encoding="utf-8") as f:
        chunks = json.load(f)

    if max_chunks:
        chunks = chunks[:max_chunks]

    # æ—¢å­˜ã®è¦ç´„ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ï¼ˆé€”ä¸­å†é–‹å¯¾å¿œï¼‰
    summaries_path = INDEX_DIR / "summaries.json"
    existing = {}
    if summaries_path.exists():
        with open(summaries_path, encoding="utf-8") as f:
            existing = json.load(f)

    client, model = create_client()
    total = len(chunks)
    new_count = 0

    print(f"ðŸ“ {total} ãƒãƒ£ãƒ³ã‚¯ã®è¦ç´„ã‚’ç”Ÿæˆä¸­ï¼ˆæ—¢å­˜: {len(existing)} ä»¶ï¼‰...")

    for i, chunk in enumerate(chunks):
        chunk_id = chunk["chunk_id"]

        # æ—¢ã«è¦ç´„æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if chunk_id in existing:
            continue

        text = chunk["text"][:500]  # è¦ç´„å¯¾è±¡ã¯å…ˆé ­500æ–‡å­—
        prompt = SUMMARY_PROMPT.format(text=text)

        try:
            response = chat(
                client, model,
                [{"role": "user", "content": prompt}],
                tools=None,
            )
            summary = (response.content or "").strip()
            existing[chunk_id] = summary
            new_count += 1

            if new_count % 50 == 0:
                print(f"  [{i+1}/{total}] {new_count} ä»¶ç”Ÿæˆæ¸ˆã¿...")
                # å®šæœŸä¿å­˜
                with open(summaries_path, "w", encoding="utf-8") as f:
                    json.dump(existing, f, ensure_ascii=False, indent=1)

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(0.1)

        except Exception as e:
            print(f"  Warning: {chunk_id} ã®è¦ç´„ç”Ÿæˆå¤±æ•—: {e}")
            time.sleep(1)
            continue

    # æœ€çµ‚ä¿å­˜
    with open(summaries_path, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=1)

    # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    _build_summary_bm25(chunks, existing)

    print(f"âœ… è¦ç´„ç”Ÿæˆå®Œäº†: {len(existing)} ä»¶ï¼ˆæ–°è¦: {new_count}ï¼‰")


def _build_summary_bm25(chunks: list, summaries: dict):
    """è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã®BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
    tagger = Tagger()

    tokenized = []
    for chunk in chunks:
        summary = summaries.get(chunk["chunk_id"], "")
        tokens = [w.surface for w in tagger(summary) if len(w.surface) > 1 or not w.surface.isascii()]
        tokenized.append(tokens)

    tokens_path = INDEX_DIR / "summary_tokens.json"
    with open(tokens_path, "w", encoding="utf-8") as f:
        json.dump(tokenized, f, ensure_ascii=False)


def _load_summary_index():
    """è¦ç´„BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰"""
    global _bm25_summary, _summaries, _chunks, _tagger

    if _bm25_summary is not None:
        return

    summaries_path = INDEX_DIR / "summaries.json"
    tokens_path = INDEX_DIR / "summary_tokens.json"
    chunks_path = INDEX_DIR / "chunks.json"

    if not summaries_path.exists() or not tokens_path.exists():
        raise FileNotFoundError("è¦ç´„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªæ§‹ç¯‰ã§ã™ã€‚")

    with open(summaries_path, encoding="utf-8") as f:
        _summaries = json.load(f)

    with open(tokens_path, encoding="utf-8") as f:
        tokenized = json.load(f)

    with open(chunks_path, encoding="utf-8") as f:
        _chunks = json.load(f)

    _bm25_summary = BM25Okapi(tokenized)
    _tagger = Tagger()


def search(query: str, top_k: int = 5, doc_filter: str | None = None) -> list[dict]:
    """è¦ç´„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§BM25æ¤œç´¢"""
    _load_summary_index()

    tokens = [w.surface for w in _tagger(query) if len(w.surface) > 1 or not w.surface.isascii()]
    if not tokens:
        return []

    scores = _bm25_summary.get_scores(tokens)
    scored = list(enumerate(scores))

    if doc_filter:
        scored = [(i, s) for i, s in scored if doc_filter in _chunks[i]["doc_id"]]

    scored.sort(key=lambda x: x[1], reverse=True)

    results = []
    for idx, score in scored[:top_k]:
        if score <= 0:
            break
        chunk = _chunks[idx]
        results.append({
            "doc_id": chunk["doc_id"],
            "chunk_id": chunk["chunk_id"],
            "filename": chunk["filename"],
            "text": chunk["text"],
            "summary": _summaries.get(chunk["chunk_id"], ""),
            "score": round(float(score), 4),
            "method": "summary_bm25",
        })

    return results


def is_available() -> bool:
    """è¦ç´„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã‹"""
    return (INDEX_DIR / "summaries.json").exists() and (INDEX_DIR / "summary_tokens.json").exists()


if __name__ == "__main__":
    import sys
    max_c = int(sys.argv[1]) if len(sys.argv) > 1 else None
    build_summaries(max_chunks=max_c)
