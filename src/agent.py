"""ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ— + CLI - å…¨Phaseçµ±åˆ"""

import sys
import json
from src.llm_client import create_client, chat
from src.tools import TOOL_DEFINITIONS, execute_tool
from src.config import MAX_TURNS
from src.prompt_builder import build_system_prompt
from src.session import (
    generate_session_id, save_session, load_session,
    list_sessions, get_latest_session_id,
)
from src.context import compress_context, estimate_messages_tokens
from src.memory import append_memory
from src.meta_agent import process_teach
from src.validator import run_validation
from src.knowledge import get_all_knowledge_summary
from src.task_planner import (
    should_use_planner, create_plan_prompt, parse_plan_response,
    create_verify_prompt, parse_verify_response, create_synthesis_prompt,
    TaskPlan, TaskStep, TaskStatus,
)


def run_agent_loop(client, model, messages: list) -> str | None:
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—: LLMå‘¼ã³å‡ºã— â†’ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ â†’ ç¹°ã‚Šè¿”ã—"""
    for turn in range(MAX_TURNS):
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®ãƒã‚§ãƒƒã‚¯
        messages = compress_context(client, model, messages)

        response = chat(client, model, messages, tools=TOOL_DEFINITIONS)

        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹å ´åˆ
        if response.tool_calls:
            messages.append(response.model_dump())

            for tc in response.tool_calls:
                fn_name = tc.function.name
                fn_args = tc.function.arguments
                print(f"  ğŸ”§ {fn_name}({_summarize_args(fn_args)})")

                result = execute_tool(fn_name, fn_args)

                messages.append({
                    "role": "tool",
                    "tool_call_id": tc.id,
                    "content": result,
                })

            continue

        # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã®å ´åˆ â†’ ãƒ«ãƒ¼ãƒ—çµ‚äº†
        content = response.content or ""
        messages.append({"role": "assistant", "content": content})
        return content

    return "(æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã«é”ã—ã¾ã—ãŸ)"


def run_planned_execution(question: str, client, model: str, max_replan: int = 2) -> str:
    """Plan-Verify-Execute ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®å®Ÿè¡Œ

    1. LLMã«è¨ˆç”»ã‚’ä½œæˆã•ã›ã‚‹
    2. å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã€æ¤œè¨¼ã™ã‚‹
    3. å…¨ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†å¾Œã€çµæœã‚’çµ±åˆã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆ

    Args:
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        model: ãƒ¢ãƒ‡ãƒ«å
        max_replan: å†è¨ˆç”»ã®æœ€å¤§å›æ•°

    Returns:
        æœ€çµ‚å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
    """
    # --- Phase 1: Plan ---
    print("  ğŸ“‹ è¨ˆç”»ã‚’ä½œæˆä¸­...")
    plan_prompt = create_plan_prompt(question)
    plan_response = chat(
        client, model,
        [{"role": "user", "content": plan_prompt}],
        tools=None,
    )
    plan = parse_plan_response(plan_response.content or "", question)

    if not plan.steps:
        # è¨ˆç”»ä½œæˆã«å¤±æ•—ã—ãŸå ´åˆã€ç›´æ¥å›ç­”ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print("  âš ï¸  è¨ˆç”»ä½œæˆå¤±æ•—ã€ç›´æ¥å›ç­”ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return None

    print(f"  ğŸ“‹ è¨ˆç”»: {plan.goal}")
    print(f"     ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(plan.steps)}")
    plan.status = TaskStatus.IN_PROGRESS

    # --- Phase 2: Execute & Verify ---
    replan_count = 0
    step_index = 0

    while step_index < len(plan.steps):
        step = plan.steps[step_index]
        step.status = TaskStatus.IN_PROGRESS
        print(f"  â–¶ ã‚¹ãƒ†ãƒƒãƒ— {step_index + 1}/{len(plan.steps)}: {step.description}")

        # Execute: search or direct LLM query
        step_result = _execute_step(step, question, client, model)
        step.result = step_result

        if not step_result:
            print(f"    âš ï¸  çµæœãªã—ã€ã‚¹ã‚­ãƒƒãƒ—")
            step.status = TaskStatus.FAILED
            step_index += 1
            continue

        # Verify
        verify_prompt = create_verify_prompt(plan, step_index)
        verify_response = chat(
            client, model,
            [{"role": "user", "content": verify_prompt}],
            tools=None,
        )
        verification = parse_verify_response(verify_response.content or "")
        step.verification = verification.get("reason", "")

        if verification["proceed"] == "replan" and replan_count < max_replan:
            # Re-plan: create a new plan incorporating what we learned
            print(f"    ğŸ”„ å†è¨ˆç”» ({replan_count + 1}/{max_replan})")
            replan_count += 1
            completed_info = _gather_completed_results(plan)
            replan_prompt = create_plan_prompt(
                f"{question}\n\n[ã“ã‚Œã¾ã§ã«å¾—ãŸæƒ…å ±]\n{completed_info}\n\n"
                f"[å†è¨ˆç”»ã®ç†ç”±] {verification.get('reason', 'ä¸æ˜')}"
            )
            replan_response = chat(
                client, model,
                [{"role": "user", "content": replan_prompt}],
                tools=None,
            )
            new_plan = parse_plan_response(replan_response.content or "", question)
            if new_plan.steps:
                # Keep completed steps, replace remaining
                completed_steps = [s for s in plan.steps if s.status == TaskStatus.COMPLETED]
                plan.goal = new_plan.goal or plan.goal
                plan.success_criteria = new_plan.success_criteria or plan.success_criteria
                plan.steps = completed_steps + new_plan.steps
                step_index = len(completed_steps)
                print(f"    ğŸ“‹ æ–°ã—ã„è¨ˆç”»: {len(new_plan.steps)} ã‚¹ãƒ†ãƒƒãƒ—è¿½åŠ ")
                continue
            else:
                # Re-plan failed, continue with original
                step.status = TaskStatus.COMPLETED
                step_index += 1
        elif verification["proceed"] == "no":
            print(f"    â­ï¸  ä¸è¦ãªçµæœã€ã‚¹ã‚­ãƒƒãƒ—: {verification.get('reason', '')}")
            step.status = TaskStatus.FAILED
            step_index += 1
        else:
            # proceed == "yes"
            print(f"    âœ… æ¤œè¨¼OK")
            step.status = TaskStatus.COMPLETED
            step_index += 1

    # --- Phase 3: Synthesize ---
    completed_count = sum(1 for s in plan.steps if s.status == TaskStatus.COMPLETED)
    if completed_count == 0:
        print("  âš ï¸  æœ‰åŠ¹ãªçµæœãªã—ã€ç›´æ¥å›ç­”ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return None

    print("  ğŸ“ æœ€çµ‚å›ç­”ã‚’çµ±åˆä¸­...")
    synthesis_prompt = create_synthesis_prompt(plan)
    synthesis_response = chat(
        client, model,
        [{"role": "user", "content": synthesis_prompt}],
        tools=None,
    )
    plan.status = TaskStatus.COMPLETED
    return synthesis_response.content or ""


def _execute_step(step: TaskStep, original_question: str, client, model) -> str | None:
    """1ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™"""
    if step.search_query:
        # Search using guided_retrieval if available, otherwise hybrid_search
        try:
            from src.guided_retrieval import guided_search
            search_result = guided_search(
                step.search_query,
                top_k=3,
                client=client,
                model=model,
            )
            results = search_result.get("results", [])
        except Exception:
            try:
                from src.hybrid_search import hybrid_search
                results, _ = hybrid_search(
                    step.search_query,
                    top_k=3,
                    doc_filter=step.doc_filter,
                    client=client,
                    model=model,
                )
            except Exception:
                results = []

        if results:
            # Format search results as text
            parts = []
            for r in results[:3]:
                doc_id = r.get("doc_id", "?")
                text = r.get("text", "")[:500]
                score = r.get("score", 0)
                parts.append(f"[{doc_id}] (score={score:.2f})\n{text}")
            return "\n---\n".join(parts)
        else:
            return None
    else:
        # No search needed - ask LLM directly for this step
        step_prompt = (
            f"å…ƒã®è³ªå•: {original_question}\n\n"
            f"ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\n{step.description}\n\n"
            f"æœŸå¾…ã™ã‚‹å‡ºåŠ›: {step.expected_output}\n\n"
            f"ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ï¼ˆ200æ–‡å­—ä»¥å†…ï¼‰ã€‚"
        )
        response = chat(
            client, model,
            [{"role": "user", "content": step_prompt}],
            tools=None,
        )
        return response.content or None


def _gather_completed_results(plan: TaskPlan) -> str:
    """å®Œäº†æ¸ˆã¿ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’åé›†"""
    parts = []
    for i, step in enumerate(plan.steps):
        if step.status == TaskStatus.COMPLETED and step.result:
            parts.append(f"ã‚¹ãƒ†ãƒƒãƒ—{i+1}: {step.description}\nçµæœ: {step.result[:200]}")
    return "\n\n".join(parts) if parts else "ãªã—"


def _summarize_args(args_json: str) -> str:
    """ãƒ„ãƒ¼ãƒ«å¼•æ•°ã‚’çŸ­ãè¡¨ç¤ºç”¨ã«ã¾ã¨ã‚ã‚‹"""
    try:
        args = json.loads(args_json)
        parts = []
        for k, v in args.items():
            s = str(v)
            if len(s) > 40:
                s = s[:37] + "..."
            parts.append(f"{k}={s}")
        return ", ".join(parts)
    except Exception:
        return args_json[:60]


def _handle_command(cmd: str, client, model) -> str | None:
    """ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚³ãƒãƒ³ãƒ‰ã‚’å‡¦ç†ã€‚å‡¦ç†ã—ãŸå ´åˆã¯çµæœæ–‡å­—åˆ—ã‚’è¿”ã™"""
    if cmd.startswith("/teach "):
        instruction = cmd[7:].strip()
        if not instruction:
            return "ä½¿ã„æ–¹: /teach <æŒ‡ç¤º>"
        return process_teach(client, model, instruction)

    elif cmd == "/validate":
        return run_validation()

    elif cmd == "/knowledge":
        return get_all_knowledge_summary()

    elif cmd == "/sessions":
        sessions = list_sessions()
        if not sessions:
            return "ä¿å­˜ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ã‚ã‚Šã¾ã›ã‚“"
        lines = ["æœ€è¿‘ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³:"]
        for s in sessions:
            lines.append(f"  {s['session_id']} ({s['message_count']} messages, {s['saved_at']})")
        return "\n".join(lines)

    elif cmd.startswith("/remember "):
        entry = cmd[10:].strip()
        if entry:
            append_memory(f"- {entry}")
            return f"ğŸ“ è¨˜æ†¶ã—ã¾ã—ãŸ: {entry}"
        return "ä½¿ã„æ–¹: /remember <è¨˜æ†¶ã™ã‚‹å†…å®¹>"

    elif cmd == "/help":
        return (
            "ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§:\n"
            "  /teach <æŒ‡ç¤º>  - ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãƒ»ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ \n"
            "  /validate      - æ–‡æ›¸ãƒ»ãƒŠãƒ¬ãƒƒã‚¸ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯\n"
            "  /knowledge     - ç™»éŒ²æ¸ˆã¿ãƒŠãƒ¬ãƒƒã‚¸ã®ä¸€è¦§è¡¨ç¤º\n"
            "  /sessions      - ä¿å­˜æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§\n"
            "  /remember <å†…å®¹> - è¨˜æ†¶ã«è¿½è¨˜\n"
            "  /help          - ã“ã®ãƒ˜ãƒ«ãƒ—\n"
            "  exit           - çµ‚äº†"
        )

    return None


def run_single(question: str) -> str:
    """1ã¤ã®è³ªå•ã‚’å‡¦ç†ã—ã¦å›ç­”ã‚’è¿”ã™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    client, model = create_client()
    system_prompt = build_system_prompt(question)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question},
    ]
    return run_agent_loop(client, model, messages)


def main():
    """CLI ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import argparse
    parser = argparse.ArgumentParser(description="Claude Helper Agent")
    parser.add_argument("--continue", dest="continue_session", action="store_true",
                        help="å‰å›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å¾©å…ƒ")
    parser.add_argument("--resume", type=str, help="æŒ‡å®šã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å¾©å…ƒ")
    args = parser.parse_args()

    client, model = create_client()
    session_id = generate_session_id()
    messages = []

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³å¾©å…ƒ
    if args.continue_session:
        last_id = get_latest_session_id()
        if last_id:
            messages, meta = load_session(last_id)
            session_id = last_id
            print(f"ğŸ“‚ ã‚»ãƒƒã‚·ãƒ§ãƒ³å¾©å…ƒ: {last_id} ({len(messages)} messages)")
        else:
            print("âš ï¸  å¾©å…ƒå¯èƒ½ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“")
    elif args.resume:
        try:
            messages, meta = load_session(args.resume)
            session_id = args.resume
            print(f"ğŸ“‚ ã‚»ãƒƒã‚·ãƒ§ãƒ³å¾©å…ƒ: {args.resume} ({len(messages)} messages)")
        except FileNotFoundError:
            print(f"âš ï¸  ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.resume}")

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒãªã‘ã‚Œã°è¿½åŠ 
    if not messages or messages[0].get("role") != "system":
        system_prompt = build_system_prompt()
        messages.insert(0, {"role": "system", "content": system_prompt})

    print(f"ğŸ¤– Agent ready ({model})")
    print(f"   Session: {session_id}")
    print("   '/help' ã§ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§, 'exit' ã§çµ‚äº†\n")

    try:
        while True:
            try:
                user_input = input("> ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")
                break

            if not user_input:
                continue
            if user_input.lower() in ("exit", "quit", "çµ‚äº†"):
                break

            # ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
            if user_input.startswith("/"):
                result = _handle_command(user_input, client, model)
                if result is not None:
                    print(f"\n{result}\n")
                    continue

            # è³ªå•ã«å¿œã˜ã¦ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‹•çš„ã«æ›´æ–°
            new_system = build_system_prompt(user_input)
            messages[0] = {"role": "system", "content": new_system}

            messages.append({"role": "user", "content": user_input})
            print()

            # Plan-Verify-Execute ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ¤å®š
            if should_use_planner(user_input):
                answer = run_planned_execution(user_input, client, model)
                if answer:
                    # è¨ˆç”»å®Ÿè¡Œã®çµæœã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã«è¿½åŠ 
                    messages.append({"role": "assistant", "content": answer})
                    print(f"\n{answer}\n")
                    continue
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¨ˆç”»å¤±æ•—æ™‚ã¯é€šå¸¸ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã¸

            answer = run_agent_loop(client, model, messages)
            if answer:
                print(f"\n{answer}\n")

    finally:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜
        if len(messages) > 1:
            save_session(session_id, messages)
            print(f"ğŸ’¾ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜: {session_id}")
        print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™")


if __name__ == "__main__":
    main()
